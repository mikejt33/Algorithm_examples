Gradient descent takes a step in the direction of the negative gradient of the function at the current point. Conjugate gradient insists descent directions be conjugate to each other and the next search direction be based on previous search directions. The Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm is a quasi-Newton method that approximates the inverse Hessian matrix and steps to the bottom of an approximated bowl. Instead of storing the inverse Hessian, Limited memory-BFGS maintains a history of the past m<10 updates of the position and gradient. These updates are used to do operations requiring the inverse Hessian-vector product.
